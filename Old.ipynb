{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input and output paths\n",
    "fda_member = \"Lee Zheng\"\n",
    "portfolio_mapping_file = r\"G:\\Corporate Logistics & Services\\General\\Forecast and Data Analytics Team\\4. Dp4a\\SAP Development\\CG Porfolio Mapping.xlsx\"\n",
    "comp_lookup_path = r\"G:\\Corporate Logistics & Services\\General\\Forecast and Data Analytics Team\\Steady-state Demands\\Master Dashboard v2\\Dashboard Extracts\\comp_mapping.csv\"\n",
    "product_info_file = r\"G:\\Corporate Logistics & Services\\General\\Forecast and Data Analytics Team\\Steady-state Demands\\Master Dashboard v2\\Dashboard Extracts\\prod_info.csv\"\n",
    "portfolio_output = r\"C:\\Users\\lzheng\\OneDrive - ALDI-HOFER\\Desktop\\portfolio_lee.csv\"\n",
    "\n",
    "# Read component\n",
    "comp_df = pd.read_csv(comp_lookup_path, usecols=[\"DISPLAY\", \"COMPONENT\"]).rename(columns={\"DISPLAY\": \"ARTICLE\"})\n",
    "\n",
    "# Find Lee's products through mapping merch node to product info for article\n",
    "mapping_df = pd.read_excel(portfolio_mapping_file, sheet_name=\"Details\", usecols=[\"FDA\", \"MERCH_CAT_NODE\"])\n",
    "mapping_filtered = mapping_df[mapping_df[\"FDA\"] == fda_member]\n",
    "merch_list = mapping_filtered[\"MERCH_CAT_NODE\"].unique().tolist()\n",
    "\n",
    "# Read product file and take useful columns\n",
    "product_df = pd.read_csv(\n",
    "    product_info_file, \n",
    "    usecols=[\n",
    "        \"DISPLAY\", \"DESCRIPTION\", \"CLASS\", \"SELLINGCLASS\", \"PRODUCTCLASS\", \n",
    "        \"COMM_GROUP_DESC\", \"CAT_DESC\", \"CG_DESC\", \"SCG_DESC\", \"MERCH_CAT_NODE\"\n",
    "    ]\n",
    ").rename(columns={\"DISPLAY\": \"ARTICLE\"})\n",
    "\n",
    "# Strip leading zeros from MERCH_CAT_NODE\n",
    "product_df['MERCH_CAT_NODE'] = product_df['MERCH_CAT_NODE'].astype(str).str.strip().str.lstrip('0')\n",
    "\n",
    "# Ensure merch_list values are strings and strip any spaces\n",
    "merch_list = [str(x).strip().lstrip('0') for x in merch_list]\n",
    "\n",
    "# Filter product_df to include only rows with MERCH_CAT_NODE in merch_list\n",
    "condition_article = product_df[\"MERCH_CAT_NODE\"].isin(merch_list)\n",
    "product_df_filtered = product_df[condition_article]\n",
    "\n",
    "# Combine tables for portfolio file\n",
    "portfolio_df = product_df_filtered.merge(comp_df, on=\"ARTICLE\", how=\"inner\")\n",
    "\n",
    "# Sort the DataFrame by 'FORECAST' and 'FC_PRICE' in descending order\n",
    "portfolio_df_sorted = portfolio_df.sort_values(by=['FORECAST', 'FC_PRICE'], ascending=[False, False])\n",
    "\n",
    "# Save the resulting DataFrame to a CSV file\n",
    "portfolio_df_sorted.to_csv(portfolio_output, index=False)\n",
    "\n",
    "# Print the first few rows of the resulting DataFrame\n",
    "print(portfolio_df_sorted.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation\n",
    "promo_df = promo_df.groupby(['FDA', 'DISPLAY', 'REGION', 'DESCRIPTION', 'SELLINGCLASS', 'CAT_DESC', 'CG_DESC', 'DISPLAY_STATUS_STRING']).agg({\n",
    "    'DIF_COUNT_STORE': 'sum',\n",
    "    'SMAPE': 'mean',\n",
    "    'APE %': 'mean',\n",
    "    'FLAG_COUNT': 'mean',\n",
    "    'BIAS %': 'mean',\n",
    "    'UNDER_BIAS': 'sum',\n",
    "    'OVER_BIAS': 'sum',\n",
    "    'DATE': ['min', 'max'],  # Get the earliest and latest dates\n",
    "    # If there are other columns not included in the groupby, you can keep the first value, for example:\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------Break CSV-----------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = r\"C:\\Users\\lzheng\\OneDrive - ALDI-HOFER\\Desktop\\ABS adjustments\\16-09\\Catchweight_10.09.xlsx\"\n",
    "df = pd.read_excel(file_path)  # Use pd.read_excel for Excel files\n",
    "\n",
    "# Determine the number of rows per split file\n",
    "num_rows = len(df)\n",
    "num_files = 20\n",
    "rows_per_file = num_rows // num_files + (num_rows % num_files > 0)\n",
    "\n",
    "# Loop through and create smaller files\n",
    "for i in range(num_files):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = (i + 1) * rows_per_file\n",
    "    df_chunk = df.iloc[start_row:end_row]\n",
    "    \n",
    "    # Save each chunk to a new CSV file with headers\n",
    "    output_file = rf'C:\\Users\\lzheng\\OneDrive - ALDI-HOFER\\Desktop\\ABS adjustments\\16-09\\Catchweight_NewConfig_{i+1}.csv'\n",
    "    df_chunk.to_csv(output_file, index=False, header=True)\n",
    "\n",
    "print(f\"CSV file split into {num_files} smaller files successfully, each with headers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------Store Order and Merge----------------------------------\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add your custom function path\n",
    "sys.path.insert(0, r\"C:\\Git\\fda\\python_functions\")\n",
    "import extract_functions as extract  # type: ignore\n",
    "\n",
    "# Establish the database connection\n",
    "conn = extract.connect_to_NDS()\n",
    "\n",
    "# Define the output file path\n",
    "order_file = r\"C:\\Users\\lzheng\\OneDrive - ALDI-HOFER\\Desktop\\Problem Statement-Markdowns\\Store Order.csv\"\n",
    "\n",
    "# Assuming you have a DataFrame `portfolio_df` that contains the list of articles\n",
    "# Generate the article list and format it for SQL\n",
    "article_list = portfolio_df['ARTICLE'].unique().tolist()\n",
    "article_list_str = ', '.join([f\"'{article}'\" for article in article_list])\n",
    "\n",
    "# SQL Query with embedded article list\n",
    "sql_query = f\"\"\"\n",
    "WITH\n",
    "--get purchase document information this will provide proposed POs and their associated dates and stores\n",
    "--PK is PDOC_NO\n",
    "pdoc AS \n",
    "    (SELECT DISTINCT\n",
    "    ekko.EBELN AS PDOC_NO,\n",
    "    CAST(eket.EINDT AS DATE) AS DELIVERY_DATE ,\n",
    "    EKPO.EBELP AS ITEM_NO,\n",
    "    ltrim(EKPO.MATNR,0) AS DISPLAY,\n",
    "    EKPO.WERKS AS SITE,\n",
    "    ekko.LIFNR AS Region,\n",
    "    EKPO.KTMNG AS TARGET_QTY,\n",
    "    EKPO.MENGE AS PO_QTY,\n",
    "    EKPO.MEINS AS PO_UOM\n",
    "    --header\n",
    "    FROM CONSUMPTION_AU.AU_E41_EKKO_CC_V ekko\n",
    "    --detail\n",
    "    INNER JOIN CONSUMPTION_AU.AU_e41_ekpo_CC_V ekpo ON ekko.EBELN=ekpo.EBELN\n",
    "    --schedule agreement\n",
    "    INNER JOIN CONSUMPTION_AU.AU_e41_eket_CC_V eket ON ekko.EBELN=eket.EBELN AND eket.ebelp=ekpo.ebelp\n",
    "    \n",
    "    ORDER BY  CAST(eket.EINDT AS DATE) desc)\n",
    ",\n",
    "--get actual delivered data at store level\n",
    "del_data AS \n",
    "    (SELECT \n",
    "    lips.VGBEL AS DOC_NO_REF_DOC,\n",
    "    LIPS.VBELN AS DELIVERY,\n",
    "    LIPS.POSNR AS DEL_ITEM,\n",
    "    CAST(LIKP.LFDAT AS DATE) AS DELIVERY_DATE,\n",
    "    ltrim(LIPS.MATNR,0) AS MATNR,\n",
    "    LIPS.MATKL AS MERCH_CAT,\n",
    "    LIPS.WERKS AS SITE,\n",
    "    LIPS.ERNAM AS USER_CREATED,\n",
    "    CAST(LIPS.ERDAT AS DATE) AS RECORD_CREATION_DATE,\n",
    "    LIPS.LFIMG AS ACTUAL_DELIVERED\n",
    "    \n",
    "    FROM CONSUMPTION_AU.AU_e41_likp_CC_V LIKP\n",
    "    INNER JOIN CONSUMPTION_AU.AU_e41_LIPS_CC_V LIPS ON LIKP.VBELN=LIPS.VBELN\n",
    "    WHERE 1=1\n",
    "    AND LIPS.WERKS NOT LIKE 'GD%%'\n",
    "    ORDER BY  CAST(LIKP.LFDAT  AS DATE) desc)\n",
    "\n",
    "SELECT DISTINCT\n",
    "pdoc.delivery_date AS DELIVERY_DATE,\n",
    "pdoc.matnr AS DISPLAY,\n",
    "pdoc.site AS SITE,\n",
    "pdoc.region AS REGION,\n",
    "sum(pdoc.PO_QTY) AS ORDERED,\n",
    "sum(del_data.actual_delivered) AS RECEIVED\n",
    "\n",
    "FROM pdoc \n",
    "FULL OUTER JOIN del_data \n",
    "    ON pdoc.PDOC_NO=del_data.DOC_NO_REF_DOC AND del_data.matnr=pdoc.matnr\n",
    "WHERE 1=1\n",
    "AND pdoc.matnr IN ({article_list_str})\n",
    "AND pdoc.region = 'GD07'\n",
    "AND pdoc.delivery_date BETWEEN '2024-07-22' AND '2024-08-18'\n",
    "GROUP BY pdoc.delivery_date,\n",
    "pdoc.matnr,\n",
    "pdoc.site,\n",
    "pdoc.region\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and fetch the data into a DataFrame\n",
    "order_df = pd.read_sql_query(sql_query, conn)\n",
    "\n",
    "# Retrieve 'SALES_UNIT_FACTOR' from the forecast dataframe\n",
    "fc_subset = fc_df[['DISPLAY', 'SALES_UNIT_FACTOR']]\n",
    "merged_df = order_df.merge(fc_subset, on='DISPLAY', how='left')\n",
    "\n",
    "# Convert cases to units\n",
    "merged_df['POQTY'] = merged_df['ORDERED'] * merged_df['SALES_UNIT_FACTOR']\n",
    "merged_df['RECD'] = merged_df['RECEIVED'] * merged_df['SALES_UNIT_FACTOR']\n",
    "\n",
    "# Select only the necessary columns\n",
    "order_df = merged_df[['DELIVERY_DATE', 'DISPLAY', 'SITE', 'REGION', 'POQTY', 'RECD']]\n",
    "\n",
    "# Save the result to a CSV file\n",
    "order_df.to_csv(order_file, index=False)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------Forecast---------------------------------------------------\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Define output path\n",
    "fc_file = r\"C:\\Users\\lzheng\\OneDrive - ALDI-HOFER\\Desktop\\Problem Statement-Markdowns\\Forecast.csv\"\n",
    "\n",
    "# Convert article_list to a string format for SQL condition\n",
    "article_list = portfolio_df['ARTICLE'].unique().tolist()\n",
    "article_list_str = ', '.join([f\"'{article}'\" for article in article_list])\n",
    "\n",
    "# Add your custom function path\n",
    "sys.path.insert(0, r\"C:\\Git\\fda\\python_functions\")\n",
    "import extract_functions as extract  # type: ignore\n",
    "\n",
    "# Establish the database connection\n",
    "conn = extract.connect_to_NDS()\n",
    "\n",
    "# Execute the SQL query with the article list embedded\n",
    "sql_query = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "VKBUR_WRK AS REGION,\n",
    "fcst.werks AS STORE,\n",
    "Ltrim(fcst.matnr,0) AS DISPLAY,\n",
    "aem.maktx AS DESCRIPTION,\n",
    "SALES_UNIT_FACTOR,\n",
    "to_varchar(fcst.fcstdate,'dd/mm/yyyy') AS FC_Date,\n",
    "sum(fcst.quantity) AS FC,\n",
    "sum(fcst.quantity*SALES_UNIT_FACTOR) AS UNIT_FC\n",
    "\n",
    "FROM\n",
    "    CONSUMPTION_AU.AU_E41_ZD2FTS_SFCST AS fcst\n",
    "INNER JOIN\n",
    "    CONSUMPTION_AU.AU_E41_MAKT AS aem\n",
    "    ON fcst.MATNR = aem.MATNR \n",
    "INNER JOIN\n",
    "    CONSUMPTION_AU.AU_E41_WRF1 AS wrf\n",
    "    ON fcst.mandt = wrf.mandt\n",
    "    AND fcst.werks = wrf.locnr\n",
    "INNER JOIN \n",
    "    CONSUMPTION_AU.AU_E41_MARA AS mara\n",
    "    ON fcst.mandt = mara.mandt\n",
    "    AND fcst.matnr = mara.matnr\n",
    "INNER JOIN\n",
    "    CONSUMPTION_AU.AU_E41_MARC AS marc\n",
    "    ON fcst.mandt = marc.mandt\n",
    "    AND fcst.matnr = marc.matnr \n",
    "    AND fcst.werks = marc.werks\n",
    "INNER JOIN\n",
    "    CONSUMPTION_AU.AU_E41_ZD2FBOMSUNIT AS bom\n",
    "    ON fcst.mandt = bom.mandt\n",
    "    AND fcst.matnr = bom.matnr\n",
    "\n",
    "WHERE 1=1\n",
    "AND VKBUR_WRK in ('GR07')--,'GR06')\n",
    "--AND fcst.werks = 'G175'\n",
    "AND fcst.MANDT='100'\n",
    "AND fcst.source = 'FC_MEAN'\n",
    "AND fcst.fcstdate BETWEEN '20240722000000' AND '20240818000000'\n",
    "--AND fcst.fcstdate BETWEEN add_days(current_date,-30) AND add_days(current_date,0) \n",
    "AND ltrim(fcst.matnr,0) in ({article_list_str})\n",
    "AND fcst.source = 'FC_MEAN'\n",
    "AND LEFT(marc.dismm,1) IN ('M','S')\n",
    "AND aem.SPRAS = 'E'\n",
    "\n",
    "GROUP BY\n",
    "VKBUR_WRK,\n",
    "fcst.werks,\n",
    "Ltrim(fcst.matnr,0),\n",
    "aem.maktx,\n",
    "SALES_UNIT_FACTOR,\n",
    "to_varchar(fcst.fcstdate,'dd/mm/yyyy')\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "fc_df = pd.read_sql_query(sql_query, conn)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "fc_df.to_csv(fc_file, index=False)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------Promo-------------------------\n",
    "# # Group by with aggregation, including min and max dates\n",
    "promo_df = promo_df.groupby(['FDA', 'DISPLAY', 'REGION', 'DESCRIPTION', 'SELLINGCLASS', 'CAT_DESC', 'CG_DESC']).agg({\n",
    "    'DIF_COUNT_STORE':'sum',\n",
    "    'SMAPE':'mean',\n",
    "    'APE %':'mean',\n",
    "    'FLAG_COUNT':'mean',\n",
    "    'BIAS %': 'mean',\n",
    "    'UNDER_BIAS': 'sum',\n",
    "    'OVER_BIAS': 'sum',\n",
    "    'DATE': ['min', 'max']  # Get the earliest and latest dates\n",
    "}).reset_index()\n",
    "\n",
    "# Print out columns to verify the presence of 'CAT_DESC'\n",
    "print(promo_df.columns)\n",
    "\n",
    "# Rename the multi-level columns resulting from the aggregation\n",
    "promo_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in promo_df.columns]\n",
    "\n",
    "# Print out columns again to verify the renaming process\n",
    "print(promo_df.columns)\n",
    "\n",
    "# Ensure 'CAT_DESC' exists and is correctly named after renaming\n",
    "category_condition = promo_df['CAT_DESC'] == 'Fruits & Vegetables'\n",
    "\n",
    "# Define the date range specific to \"Fruits & Vegetables\"\n",
    "end_date_produce = pd.to_datetime(datetime.today().date()) - timedelta(days=6) \n",
    "start_date_produce = pd.to_datetime(datetime.today().date()) - timedelta(days=12)\n",
    "\n",
    "# Apply the filter based on both date range and 'CAT_DESC'\n",
    "date_condition_produce = (promo_df['DATE_min'] <= end_date_produce) & (promo_df['DATE_max'] >= start_date_produce)\n",
    "\n",
    "# Apply the date filter only for 'Fruits & Vegetables'\n",
    "promo_df['Filtered'] = np.where(category_condition, date_condition_produce, True)\n",
    "\n",
    "# Filter the DataFrame based on the 'Filtered' column\n",
    "promo_df = promo_df[promo_df['Filtered']].drop(columns=['Filtered'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------DIF-----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load DIF data\n",
    "dif_path = r\"C:\\Users\\lzheng\\OneDrive - ALDI-HOFER\\Documents\\DIF.csv\"\n",
    "dif = pd.read_csv(dif_path, on_bad_lines='skip', engine='python')\n",
    "\n",
    "# Convert the 'DATE' column to datetime format\n",
    "dif['DATE'] = pd.to_datetime(dif['DATE'], errors='coerce', format='%Y-%m-%d')  \n",
    "\n",
    "# Define date range for the DIF data\n",
    "end_date = pd.to_datetime(datetime.today().date())\n",
    "start_date = end_date - timedelta(weeks=2)\n",
    "\n",
    "# Filter DIF data based on the date range\n",
    "date_condition = (dif['DATE'] <= end_date) & (dif['DATE'] >= start_date)\n",
    "dif = dif[date_condition]\n",
    "\n",
    "# Drop specific columns\n",
    "dif = dif.drop(columns=['USER_DIF_TAG', 'Unnamed: 0'])\n",
    "\n",
    "# Aggregate DIF data by DISPLAY and REGION\n",
    "grouped_dif = dif.groupby(['DISPLAY', 'REGION', 'DATE'], as_index=False).agg({'DIF_COUNT_STORE': 'sum'})\n",
    "\n",
    "# Merge DIF data with the main file\n",
    "promo_df = rank_all_region.merge(grouped_dif, on=[\"DISPLAY\", \"REGION\"], how=\"inner\")\n",
    "\n",
    "# Filter out rows with zero or NaN DIF_COUNT_STORE values\n",
    "promo_df = promo_df[(promo_df['DIF_COUNT_STORE'] != 0) & (promo_df['DIF_COUNT_STORE'].notna())]\n",
    "\n",
    "# Convert 'DATE' column to datetime\n",
    "promo_df['DATE'] = pd.to_datetime(promo_df['DATE'])\n",
    "\n",
    "# Group by with aggregation, including min and max dates\n",
    "promo_df = promo_df.groupby(['FDA', 'DISPLAY', 'REGION', 'DESCRIPTION', 'SELLINGCLASS', 'CAT_DESC', 'CG_DESC']).agg({\n",
    "    'DIF_COUNT_STORE':'sum',\n",
    "    'SMAPE':'mean',\n",
    "    'APE %':'mean',\n",
    "    'FLAG_COUNT':'mean',\n",
    "    'BIAS %': 'mean',\n",
    "    'UNDER_BIAS': 'sum',\n",
    "    'OVER_BIAS': 'sum',\n",
    "    'DATE': ['min', 'max']  # Get the earliest and latest dates\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the multi-level columns resulting from the aggregation\n",
    "promo_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in promo_df.columns]\n",
    "\n",
    "# Define the date range specific to \"Fruits & Vegetables\"\n",
    "end_date_produce = pd.to_datetime(datetime.today().date()) - timedelta(days=6) \n",
    "start_date_produce = pd.to_datetime(datetime.today().date()) - timedelta(days=12)\n",
    "\n",
    "# Apply the filter based on both date range and 'CAT_DESC'\n",
    "date_condition_produce = (promo_df['DATE_min'] <= end_date_produce) & (promo_df['DATE_max'] >= start_date_produce)\n",
    "category_condition = promo_df['CAT_DESC'] == 'Fruits & Vegetables'\n",
    "\n",
    "# Apply the date filter only for 'Fruits & Vegetables'\n",
    "promo_df['Filtered'] = np.where(category_condition, date_condition_produce, True)\n",
    "\n",
    "# Filter the DataFrame based on the 'Filtered' column\n",
    "promo_df = promo_df[promo_df['Filtered']].drop(columns=['Filtered'])\n",
    "\n",
    "# Save the result to a CSV file\n",
    "promo_df.to_csv(r\"G:\\Corporate Logistics & Services\\General\\Forecast and Data Analytics Team\\Steady-state Demands\\Master Dashboard v2\\Dashboard Extracts\\Promo.csv\", index=False)\n",
    "\n",
    "# Define the output path for the Excel file\n",
    "output_path = r\"G:\\Corporate Logistics & Services\\General\\Forecast and Data Analytics Team\\Steady-state Demands\\Master Dashboard v2\\WILO\\Extracts\\WILO_14.xlsx\"\n",
    "\n",
    "# Save final files to Excel\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    # Write rank_filter_2 DataFrame to the \"WILO_14\" sheet\n",
    "    rank_filter_2.to_excel(writer, sheet_name='WILO_14', index=False)\n",
    "    \n",
    "    # Write promo_df DataFrame to the \"Promo\" sheet\n",
    "    promo_df.to_excel(writer, sheet_name='Promo', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------SOH, going through folders-----------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base path\n",
    "base_path = r'G:\\Corporate Logistics & Services\\General\\RDA\\13.Daily_OP_DataFeed\\2024'\n",
    "# Output file path\n",
    "SOH_file = r\"C:\\Users\\lzheng\\OneDrive - ALDI-HOFER\\Desktop\\Problem Statement-Markdowns\\SOH.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "soh_df = pd.DataFrame()\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('20240818', format='%Y%m%d')\n",
    "end_date = pd.to_datetime('20240901', format='%Y%m%d')\n",
    "\n",
    "# Loop through each date in the specified range\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Format the current date to match the folder name\n",
    "    folder_name = f\"REF_{current_date.strftime('%Y%m%d')}\"\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "    \n",
    "    # Define the file path for the N-3_GD07.csv file\n",
    "    file_path = os.path.join(folder_path, 'N-3_GD07.csv')\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file and extract the required columns\n",
    "        temp_df = pd.read_csv(file_path, usecols=['PLANNING_DATE', 'SAP_DISPLAY', 'LOCATION', 'STOCK', 'SHLF_LFE_INDAYS'])\n",
    "        \n",
    "        # Append the data to the main DataFrame\n",
    "        soh_df = pd.concat([soh_df, temp_df], ignore_index=True)\n",
    "    \n",
    "    # Move to the next date\n",
    "    current_date += pd.Timedelta(days=1)\n",
    "\n",
    "# Assuming portfolio_df is defined somewhere in your script\n",
    "# Filter the article list\n",
    "article_list = portfolio_df['ARTICLE'].unique().tolist()\n",
    "condition_article = soh_df['SAP_DISPLAY'].isin(article_list)\n",
    "soh_df = soh_df[condition_article]\n",
    "\n",
    "# Create the 'INDEX' key using columns from soh_df\n",
    "soh_df['INDEX'] = soh_df['SAP_DISPLAY'] + '-' + soh_df['LOCATION'] + '-' + soh_df['PLANNING_DATE'].astype(str)\n",
    "\n",
    "# Save the final DataFrame\n",
    "soh_df.to_csv(SOH_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
